---
title: "Analysis and Comparison of Multiple Regression Methods for Credit Data"
author: "Liang Hao, Andrew Shibata"
date: "11/4/2016"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
options(knitr.comment = FALSE)
library(glmnet)
library(pls)
library(xtable)
```

## Introduction
These are the accompanying slides to our project which bears the same title.

In the following slides, we would introduce the data set, goal, methods, results and conclusions for the project.

## Data
The data for this project, [Credit Data](http://www-bcf.usc.edu/~gareth/ISL/Credit.csv), come from the generous contribution of *An Introduction to Statistical Learning*.


|Predictor|Predictor|Response|
|-----|-----|-----|
|Quantitative|Qualitative|Quantitative|
|Income, Limit, Rating, Cards, Age, Education|Gender, Student, Married, Ethnicity|Balance|


## Goal

The goal of this project is thus to choose and fit a regression model in order to predict the *Balance* of any new client with the above information as input.

Because of the numeric nature of the dependent variable *Balance*, this is a *Regression* Problem.

## Methods
The regression methods that we will use in this project include:

* Ordianry Linear Squares (OLS)

### Shrinkage Methods

- Ridge Regression (RR)
  
- Lasso Regression (LR)

### Dimensional Reduction Methods

- Principal Components Regression (PCR)
  
- Partial Least Squares Regression (PLSR)


## Shrinkage Methods:
### Ridge Regression
$$\sum_{i = 1}^{n}(y_{i} - \beta_{0} - \sum_{j = 1}^{p}\beta_{j}x_{ij})^{2} + \lambda \sum_{j = 1}^{p}\beta_{j}^{2} = RSS  + \lambda \sum_{j = 1}^{p}\beta_{j}^{2} $$

### Lasso Regression
$$\sum_{i = 1}^{n}(y_{i} - \beta_{0} - \sum_{j = 1}^{p}\beta_{j}x_{ij})^{2} + \lambda \sum_{j = 1}^{p}|\beta_{j}| = RSS + \lambda \sum_{j = 1}^{p}|\beta_{j}|$$


## Dimensional Reduction Methods:
### Principal Components Regression

- Based on Principal Component Analysis (PCA)

- Regressing the principal components of the explanatory variables instead of the varaibles directly.

### Partial Least Squares Regression

- Related to PCR

- Instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space.

## Cross-validation (CV)

- Definition: Model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set

- Purpose: define a dataset to "test" the model in the training phase, in order to limit problems like overfitting, give an insight on how the model will generalize to the test dataset, etc.

- Summary: combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.


## Results: Ridge vs. Lasso
```{r, echo = FALSE, message=FALSE, fig.width=9, fig.height=3.3, eval=TRUE}
load('../data/regressionLR-cvResult.RData')
load('../data/regressionRR-cvResult.RData')
par(mfrow=c(1,2))
plot(rr.cv.out, main = '10-fold CV for Ridge')
plot(lr.cv.out, main = '10-flod CV for Lasso')
par(mfrow=c(1,1))
```

|Methods|Lambda|
|----|----|
|Ridge|0.01321941|
|Lasso|0.01|


## Results
### PCR vs. PLSR

## Conclusions
In the end, we may conclude that 

## After Thoughts
In the analysis, we also realized that ....

Therefore, in the future, 




## Slide with R Output


## Slide with Plot



