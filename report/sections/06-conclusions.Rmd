---
title: "Conclusions"
author: "Liang Hao, Andrew Shibata"
date: "11/1/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
options(knitr.comment = FALSE)
```

The ultimate goal of this project was to develop methods for minimizing MSE on the test sets across different regression methods. Each of the *Principle Component*, *Partial Least Squares*, *Lasso*, and *Ridge* regression models optimizes MSEP differently. While *PCR* and *PSLR* are more heavily rooted in components, *LR* and *RR* also distinguish each other through subsetting the predictive elements used for modeling. *LR* seems to prefer a small amount of tuning, while the other 3 prefer more drastic effect sizes whether it be through the lambda tuning factor or through using a larger number of components. In this sense, the best regression model can be determined by considering how many possible components there are and how finely tuned the model should be to improve on ordinary least squares regression.